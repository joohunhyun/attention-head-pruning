model_name: bert-base-uncased
num_labels: 2
batch_size: 16
learning_rate: 2e-5
prune_ratio: 0.2
